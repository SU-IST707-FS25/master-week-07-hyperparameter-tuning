{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5aad0a1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, we will use optimization to modulate the behavior of a clustering algorithm for processing text data.  This closely mirrors work in the lecture; you should be able to copy and adapt the code there to answer lab questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77659179",
   "metadata": {},
   "source": [
    "### Data Acquision and Preprocessing\n",
    "\n",
    "For this lab, we'll use a handful of articles from NYT and Fox News on the same evolving new story.  The goal is to see if we can characterize the differences between the two news sources. Here's code for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6283753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jeintron/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jeintron/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uncomment the following to install nltk\n",
    "#%pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348b7d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id                                           sentence source\n",
      "0           0  Former-President Trump on Thursday denied a re...    FOX\n",
      "1           0  Another fake story, that I flushed papers and ...    FOX\n",
      "2           0  CNN spent much of the morning covering breakin...    FOX\n",
      "3           0          Haberman is also a CNN political analyst.    FOX\n",
      "4           0               We are beginning with breaking news.    FOX\n",
      "(339, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_articles(filename, source):\n",
    "    \"\"\"\n",
    "    Parse a text file into a list of (article_id, sentence, source).\n",
    "    \n",
    "    - filename: path to text file\n",
    "    - source: string label (\"NYT\" or \"FOX\")\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    article_id = -1\n",
    "    \n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    buffer = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # new article starts with a line beginning with '#'\n",
    "        if line.startswith(\"#\"):\n",
    "            # flush old buffer if any\n",
    "            if buffer:\n",
    "                text = \" \".join(buffer)\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                for sent in sentences:\n",
    "                    clean = re.sub(r\"['\\\"“”‘’]\", \"\", sent).strip()\n",
    "                    if clean:  # skip empty\n",
    "                        rows.append((article_id, clean, source))\n",
    "                buffer = []\n",
    "            \n",
    "            article_id += 1  # increment article\n",
    "            continue\n",
    "        \n",
    "        if line:  # skip blank lines\n",
    "            buffer.append(line)\n",
    "    \n",
    "    # flush last article\n",
    "    if buffer:\n",
    "        text = \" \".join(buffer)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for sent in sentences:\n",
    "            clean = re.sub(r\"['\\\"“”‘’]\", \"\", sent).strip()\n",
    "            if clean:\n",
    "                rows.append((article_id, clean, source))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "# Parse both sources\n",
    "fox_rows = parse_articles(\"../data/fox.txt\", \"FOX\")\n",
    "nyt_rows = parse_articles(\"../data/nyt.txt\", \"NYT\")\n",
    "\n",
    "# Combine into dataframe\n",
    "df = pd.DataFrame(fox_rows + nyt_rows, columns=[\"article_id\", \"sentence\", \"source\"])\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3908099",
   "metadata": {},
   "source": [
    "### Embed the articles and visualize\n",
    "\n",
    "Use the SentenceTransformer library with all-MiniLM-L6-v2 to derive embeddings, and then UMAP to embed the articles and look at them.  Can you distinguish between sources?  Fiddle with parameters to see how this influences your visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c06130",
   "metadata": {},
   "source": [
    "### Prepare UMAP projecttions for optimization\n",
    "\n",
    "While it is possible to use hyperparameter optimization to obtain the \"best\" projection you can given some loss function, it's faster to precompute a few embeddings and let the optimizer choose amongst them.  Prepare three embeddings for your optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83662fd5",
   "metadata": {},
   "source": [
    "### Optimize for coherence and coverage\n",
    "\n",
    "Use the code from the lab notebook to optimize for both coverage and coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559d484",
   "metadata": {},
   "source": [
    "### Visualize your results\n",
    "\n",
    "Have a look at your results from the optimizer, using the code from the lecture notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3bf58b",
   "metadata": {},
   "source": [
    "### Inspect your data\n",
    "\n",
    "Build a function to list out a sample the sentences for a given cluster, and then build another function to iterate over these clusters in order, printing out the sentences for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1d9db",
   "metadata": {},
   "source": [
    "### Identify patterns\n",
    "\n",
    "Build another visualization function that allows you to inspect the relative number of articles from either source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39991574",
   "metadata": {},
   "source": [
    "### Play with parameters!  Adjust your toolkit!\n",
    "\n",
    "Now that you've built yourself a small analytical toolkit, you can go back and adjust the code to inspect different parameter sets, or enhance its functionality. Try:\n",
    "\n",
    "- Shifting the balance between coherence and coverage\n",
    "- Changing the importance of cluster size\n",
    "- Labeling the clusters in the scatter plot\n",
    "- Algorithmically detecting the key differentiating topics.\n",
    "\n",
    "What makes sense?  What can you learn?  What might come next?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
