{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420ac191",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Clustering Algorithms\n",
    "\n",
    "## Introduction: From Manual Exploration to Automated Search\n",
    "\n",
    "In your previous work with clustering, you've manually explored parameters using loops to examine curves, conducted basic stability analysis, and used evaluation metrics like silhouette scores and Davies-Bouldin indices. While this manual approach builds intuition, it becomes impractical as the parameter space grows and computations become expensive.\n",
    "\n",
    "**The key insight**: Hyperparameter tuning tools **automate the search process** so that we can focus on designing a good **evaluation function** rather than manually trying every possibility.\n",
    "\n",
    "Different tools have different conventions for this evaluation function:\n",
    "\n",
    "- In **scikit-learn**, we pass a **scoring function** that the search will **maximize**.\n",
    "- In many other libraries — especially deep learning frameworks and general-purpose optimizers like Hyperopt, Optuna, or Bayesian optimization — we pass a **loss function** that the search will **minimize**.\n",
    "\n",
    "A scoring function and a loss function are two sides of the same coin:\n",
    "- **Score:** higher is better  \n",
    "- **Loss:** lower is better  \n",
    "\n",
    "If you have one, you can usually get the other just by multiplying by -1.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters vs. Hyperparameters in Clustering\n",
    "\n",
    "Before diving into optimization, we should be clear about what exactly we are optimizing.  **Parameters** are learned by the underlying algorithm. These are not the direct target of optimization. Examples of parameters for clustering algorithms are:\n",
    "\n",
    "- **K-Means**: centroids (cluster center coordinates)\n",
    "- **Gaussian mixture model**: means and covariances\n",
    "- **Hierarchical clustering**: linkage distances at each merge\n",
    "\n",
    "Optimization focuses on **hyperparameters**, which control how the underlying algorithm finds parameters.  Examples of hyperparameters include:\n",
    "\n",
    "- **K-Means**: Number of clusters (`k`)\n",
    "- **DBSCAN**: `epsilon` and `min_samples`\n",
    "- **Hierarchical clustering**: Linkage criterion in hierarchical clustering\n",
    "- **UMAP**: Dimensionality reduction settings (`n_neighbors` and `min_dist`)\n",
    "\n",
    "**Key takeaway**: Hyperparameter optimization algorithms let us search over _hyperparameters_ systematically instead of by hand; the _parameters_ are still determined by the underlying algorithm we are trying to optimize.  A _loss function_ (or _scoring function_) guides the search.\n",
    "\n",
    "---\n",
    "\n",
    "## The Challenge in Clustering: Designing the Evaluation Function\n",
    "\n",
    "In supervised learning, defining a loss function is usually straightforward — we measure prediction error (e.g., RMSE, cross-entropy) and minimize it.  \n",
    "\n",
    "In clustering, the challenge is: **How do we translate “good clustering” into a number we can optimize?**\n",
    "\n",
    "Possible objectives include:\n",
    "- **Coverage:** How many points are assigned to a cluster?\n",
    "- **Coherence:** How similar are points within the same cluster?\n",
    "- **Stability:** Do clusters stay consistent across different runs?\n",
    "- **Interpretability:** Can a domain expert make sense of them?\n",
    "\n",
    "The standard metrics you’ve seen before — inertia, silhouette score, Davies–Bouldin index — are **knowledge-lean**: they capture geometric structure but know nothing about your specific domain. They’re useful heuristics, not universal measures of quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Using the Elbow Method as an Evaluation Function\n",
    "\n",
    "Let’s look at a classic problem: choosing `k` in K-Means.\n",
    "\n",
    "### Why Minimizing Inertia Fails\n",
    "\n",
    "Thought experiment:\n",
    "- `k=1`: one cluster → high inertia\n",
    "- `k=2`: lower inertia\n",
    "- `k=3`: even lower\n",
    "- `k=n`: every point is its own cluster → inertia = 0\n",
    "\n",
    "If we **minimize inertia** directly, we end up with `k=n`, which is useless.  \n",
    "\n",
    "Instead, the **elbow** represents a sweet spot between fewer clusters (parsimony) and tighter clusters (low inertia).\n",
    "\n",
    "---\n",
    "\n",
    "### Turning the Elbow into a Function We Can Optimize\n",
    "\n",
    "We can capture the “elbow” by looking at the **second derivative** of inertia with respect to `k`.  \n",
    "Intuition: at the elbow, the rate of inertia improvement slows down sharply — i.e., curvature is high.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1. Implement a function to compute an “elbow score” for each `k` based on curvature.\n",
    "2. Wrap that into a **custom scorer** for `sklearn`’s `GridSearchCV` (remember: in scikit-learn, scorers are maximized).\n",
    "3. Compare our elbow-based choice of `k` with another metric: the silhouette score.\n",
    "\n",
    "---\n",
    "\n",
    "📌 **A note about this example:**  \n",
    "In most optimization settings, the evaluation function is computed for each candidate parameter setting independently.  \n",
    "Here, the elbow score depends on **multiple `k` values at once**, so we precompute inertias over a whole range.  \n",
    "This means that for our toy example, `GridSearchCV` isn’t really doing much “search” — but it’s a useful illustration of how custom scorers fit into the API.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1 — Generate some example data\n",
    "\n",
    "We’ll use `make_blobs` to create a synthetic dataset with four clusters. This gives us a “ground truth” number of clusters, but we won’t use it for tuning — it’s just to know what the real answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import make_scorer, silhouette_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=300, \n",
    "    centers=4, \n",
    "    n_features=2, \n",
    "    random_state=42, \n",
    "    cluster_std=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60a1c3",
   "metadata": {},
   "source": [
    "### Step 2 — Define a function to calculate the \"elbow score\"\n",
    "\n",
    "The elbow score is based on the **second derivative** of inertia with respect to `k`.\n",
    "\n",
    "- The **first derivative** tells us how quickly inertia is decreasing.\n",
    "- The **second derivative** tells us how much that rate of decrease is slowing down — a large positive second derivative means we’ve hit the “bend” in the curve.\n",
    "\n",
    "We’ll also smooth the inertia values with a Gaussian filter to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_elbow_score(inertias):\n",
    "    \"\"\"\n",
    "    Calculate elbow score using a smoothed second derivative.\n",
    "    Higher scores indicate stronger \"elbow\" points.\n",
    "    \"\"\"\n",
    "    # Smooth the curve to reduce noise\n",
    "    smoothed = gaussian_filter1d(inertias, sigma=0.5)\n",
    "    \n",
    "    # First derivative (rate of change)\n",
    "    first_deriv = np.gradient(smoothed)\n",
    "    \n",
    "    # Second derivative (curvature)\n",
    "    second_deriv = np.gradient(first_deriv)\n",
    "    \n",
    "    return second_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae11e59",
   "metadata": {},
   "source": [
    "### Step 3 — Create a custom scorer for GridSearchCV\n",
    "\n",
    "`GridSearchCV` in sklearn expects a **scoring function** with the signature:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53b98b",
   "metadata": {},
   "source": [
    "~~~python\n",
    "scorer(estimator, X, y=None) -> float\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20f4fe",
   "metadata": {},
   "source": [
    "Since clustering is unsupervised, we won’t use y.\n",
    "\n",
    "Our plan:\n",
    "\n",
    "- Decide on a range of possible k values.\n",
    "- Fit K-Means for each k and store the inertia.\n",
    "- Use our calculate_elbow_score function to compute elbow scores for all tested k.\n",
    "- Create a lookup dictionary so that, when GridSearchCV tries a given k, our scorer returns the corresponding elbow score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_elbow_scorer(k_range, X):\n",
    "    \"\"\"Create an elbow score function for GridSearchCV.\"\"\"\n",
    "    inertias = []\n",
    "    for k in k_range:   \n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        km.fit(X)\n",
    "        inertias.append(km.inertia_)\n",
    "    \n",
    "    elbow_scores = calculate_elbow_score(inertias)\n",
    "    elbow_lookup = dict(zip(k_range, elbow_scores))\n",
    "    \n",
    "    def elbow_scorer(estimator, X_test, y=None):\n",
    "        # GridSearchCV will fit the estimator with the given n_clusters;\n",
    "        # we just look up the precomputed elbow score for that k.\n",
    "        return elbow_lookup.get(estimator.n_clusters, 0.0)\n",
    "\n",
    "    return elbow_scorer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551900a8",
   "metadata": {},
   "source": [
    "### Step 4 — Use GridSearchCV to tune `k`\n",
    "\n",
    "Normally, `GridSearchCV` splits the data into train/test folds.  \n",
    "Since we’re not predicting labels here, we just want it to evaluate each `k` **once** on all the data.\n",
    "\n",
    "We can do this by passing a **custom CV splitter** that uses the whole dataset for both “train” and “test” in each fold.\n",
    "\n",
    "Here, we’ll search for `k` from 2 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,  PredefinedSplit\n",
    "\n",
    "# Range of k values to test\n",
    "k_values = range(2, 11)\n",
    "n = X.shape[0]\n",
    "one_fold = [(np.arange(n), np.arange(n))] # We only need one fold here\n",
    "\n",
    "# Create the grid search object\n",
    "grid = GridSearchCV(\n",
    "    estimator=KMeans(random_state=42, n_init=10),\n",
    "    param_grid={'n_clusters': k_values},\n",
    "    scoring=make_elbow_scorer(k_values, X),\n",
    "    cv=one_fold,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "grid.fit(X)\n",
    "\n",
    "print(\"Best k according to elbow score:\", grid.best_params_['n_clusters'])\n",
    "print(\"Best elbow score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c076f76",
   "metadata": {},
   "source": [
    "### Step 5 — Compare with the silhouette score\n",
    "\n",
    "The **silhouette score** is another metric for evaluating clustering quality.  \n",
    "It measures how similar points are to their own cluster compared to other clusters.\n",
    "\n",
    "We can repeat the same `GridSearchCV` process but use `silhouette_score` as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d528f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scorer = lambda est, X, y=None: silhouette_score(X, est.labels_)\n",
    "\n",
    "grid_sil = GridSearchCV(\n",
    "    estimator=KMeans(random_state=42, n_init=10),\n",
    "    param_grid={'n_clusters': k_values},\n",
    "    scoring=silhouette_scorer,\n",
    "    cv=one_fold\n",
    ")\n",
    "\n",
    "grid_sil.fit(X)\n",
    "\n",
    "print(\"Best k by silhouette:\", grid_sil.best_params_['n_clusters'])\n",
    "print(\"Best silhouette score:\", grid_sil.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3d07f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 6 — Summary\n",
    "\n",
    "- **The elbow method** is a way to encode our intuition about where adding more clusters stops helping much.\n",
    "- By turning it into a **custom scoring function**, we can plug it directly into `GridSearchCV` or `RandomizedSearchCV`.\n",
    "- In real-world problems, you might combine multiple metrics (e.g., elbow score, silhouette score) or use domain knowledge to guide hyperparameter tuning. For instance, you could \n",
    "\n",
    "This is a simple but important lesson:  \n",
    "> In unsupervised learning, *your scoring function is your definition of success*.  \n",
    "> Automated search is only as good as the way you measure “good.”\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Balancing Coverage and Coherence in DBSCAN Hyperparameter Tuning\n",
    "\n",
    "In real-world clustering, not every algorithm assigns *every* point to a cluster.  \n",
    "For example, **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) and its cousin HDBSCAN can mark points as “noise” if they don’t belong to any dense region.\n",
    "\n",
    "This creates a unique **trade-off**:\n",
    "\n",
    "- If `eps` (the neighborhood radius) is too small, many points are left as noise → **low coverage**.\n",
    "- If `eps` is too large, clusters start to merge and lose shape → **low coherence**.\n",
    "\n",
    "We often want to guide the algorithm’s parameters to strike the right balance between:\n",
    "1. **Coverage** – the proportion of points assigned to a cluster.\n",
    "2. **Coherence** – how well-separated the resulting clusters are (measured here with the silhouette score).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1 — Create a dataset with noise\n",
    "\n",
    "We’ll make a synthetic dataset using `make_blobs`, then add random noise points to simulate a messier real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f881983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Clustered points\n",
    "X_clusters, y_clusters = make_blobs(\n",
    "    n_samples=300, \n",
    "    centers=4, \n",
    "    cluster_std=0.60, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Noise points (uniformly scattered)\n",
    "X_noise = np.random.uniform(low=-8, high=8, size=(50, 2))\n",
    "\n",
    "# Combine into one dataset\n",
    "X = np.vstack([X_clusters, X_noise])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=30, alpha=0.6)\n",
    "plt.title(\"Synthetic Data: Clusters + Noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a081b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2 — Define coverage and quality\n",
    "\n",
    "**Coverage**: the fraction of points that are *not* labeled as noise by DBSCAN (`label != -1`).\n",
    "\n",
    "**Quality**: the silhouette score for the clustered points only (silhouette measures separation and compactness).  \n",
    "⚠️ *Note*: The silhouette score is biased toward convex clusters — in more complex shapes, you might choose a different metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_score(labels):\n",
    "    \"\"\"Proportion of points assigned to any cluster (label != -1).\"\"\"\n",
    "    return (labels != -1).mean()\n",
    "\n",
    "def coherence_score(X, labels):\n",
    "    \"\"\"Silhouette score for non-noise points.\"\"\"\n",
    "    mask = labels != -1\n",
    "    if mask.sum() < 2 or len(np.unique(labels[mask])) < 2:\n",
    "        return -1  # Not enough data to compute\n",
    "    return silhouette_score(X[mask], labels[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f1117",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3 — Combine coverage and coherence into a single score\n",
    "\n",
    "We’ll define a simple **weighted average**:\n",
    "\n",
    "\\[\n",
    "\\text{score} = w \\times \\text{coverage} + (1-w) \\times \\text{coherence}\n",
    "\\]\n",
    "\n",
    "- If `w` is high → we prioritize including more points (coverage).\n",
    "- If `w` is low → we prioritize cluster quality (coherence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dbscan_scorer(coverage_weight=0.5):\n",
    "    \"\"\"Return a sklearn-compatible scorer for DBSCAN.\"\"\"\n",
    "    def score(estimator, X, y=None):\n",
    "        labels = estimator.fit_predict(X)\n",
    "        cov = coverage_score(labels)\n",
    "        coh = coherence_score(X, labels)\n",
    "        if coh == -1:\n",
    "            return -1\n",
    "        return coverage_weight * cov + (1 - coverage_weight) * coh\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6d1ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4 — Search over DBSCAN parameters with GridSearchCV\n",
    "\n",
    "We’ll search over:\n",
    "- `eps`: neighborhood radius\n",
    "- `min_samples`: minimum number of points in a dense region\n",
    "\n",
    "Because this is unsupervised, we’ll use a dummy “CV” split so each parameter setting is evaluated on all the data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2793d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_values = np.linspace(0.2, 1.5, 10)\n",
    "min_samples_values = range(3, 10)\n",
    "\n",
    "n = X.shape[0]\n",
    "one_fold = [(np.arange(n), np.arange(n))] # We only need one fold here\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=DBSCAN(),\n",
    "    param_grid={'eps': eps_values, 'min_samples': min_samples_values},\n",
    "    scoring=make_dbscan_scorer(coverage_weight=0.8),\n",
    "    cv=one_fold,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid.fit(X)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc62f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 5 — Visualize the parameter search space\n",
    "\n",
    "We can reshape the results into a table so we can see how the score changes across `eps` and `min_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eda288",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(grid.cv_results_)\n",
    "pivot_table = results_df.pivot(\n",
    "    index='param_min_samples',\n",
    "    columns='param_eps',\n",
    "    values='mean_test_score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"DBSCAN Coverage–Coherence Score Across Parameter Space\")\n",
    "plt.xlabel(\"eps\")\n",
    "plt.ylabel(\"min_samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f102b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 6 — Inspect the best clustering\n",
    "\n",
    "Let’s fit DBSCAN with the best parameters and see the resulting clusters and noise points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dbscan = DBSCAN(**grid.best_params_)\n",
    "labels = best_dbscan.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "unique_labels = np.unique(labels)\n",
    "for label in unique_labels:\n",
    "    mask = labels == label\n",
    "    if label == -1:\n",
    "        color = \"k\"\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], c=color, s=20, label=\"Noise\", alpha=0.5)\n",
    "    else:\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], s=30, label=f\"Cluster {label}\")\n",
    "plt.legend()\n",
    "plt.title(\"Best DBSCAN Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e090d43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 7 — Try different coverage weights\n",
    "\n",
    "Right now, we’re giving equal weight to coverage and coherence (`coverage_weight=0.5`).  \n",
    "What happens if we:\n",
    "- Increase it to 0.8 (favoring coverage)?\n",
    "- Decrease it to 0.2 (favoring quality)?\n",
    "\n",
    "📌 **Your task**:  \n",
    "- Change `coverage_weight` in `make_dbscan_scorer`.\n",
    "- Rerun the grid search.\n",
    "- Compare the best parameters and resulting cluster shapes.\n",
    "\n",
    "---\n",
    "\n",
    "**Key takeaway**:  \n",
    "In clustering, *how you define the score* is as important as the search method.  \n",
    "Coverage vs. coherence is just one example — in your own problems, you may include stability, interpretability, or domain-specific rules in the score.\n",
    "\n",
    "\n",
    "## Understanding Search Algorithms: From Grid to Bayesian Optimization\n",
    "\n",
    "Now that we understand the importance of loss function design, let's explore different algorithms for searching the hyperparameter space.\n",
    "\n",
    "### Search Methods Comparison\n",
    "\n",
    "**Grid Search**: Exhaustive but expensive\n",
    "- **How it works**: Try every combination in a predefined grid\n",
    "- **Pros**: Guaranteed to find the best point in the grid\n",
    "- **Cons**: Exponential growth with dimensions; no learning from previous evaluations\n",
    "- **When to use**: Small parameter spaces (≤3 dimensions), unlimited compute\n",
    "\n",
    "**Random Search**: Often surprisingly effective\n",
    "- **How it works**: Randomly sample points from the parameter space\n",
    "- **Pros**: Often outperforms grid search with the same budget\n",
    "- **Cons**: No learning from previous evaluations\n",
    "- **Key insight**: Most hyperparameters don't matter much; random search finds the few that do\n",
    "\n",
    "**Bayesian Optimization with Hyperopt**: Smart exploration and exploitation\n",
    "- **How it works**: Build a probabilistic model of the objective function\n",
    "- **Pros**: Learns from previous evaluations; balances exploration vs exploitation\n",
    "- **Algorithm**: Tree-structured Parzen Estimator (TPE)\n",
    "- **When to use**: Expensive evaluations, complex parameter spaces\n",
    "\n",
    "### Hyperopt and TPE: What's Happening Under the Hood?\n",
    "\n",
    "Hyperopt uses **Tree-structured Parzen Estimator (TPE)**, a Bayesian optimization algorithm. Here's the intuition:\n",
    "\n",
    "1. **Maintain two models**:\n",
    "   - l(x): Model of parameter values that led to GOOD results\n",
    "   - g(x): Model of parameter values that led to BAD results\n",
    "\n",
    "2. **Select next point** by maximizing l(x)/g(x)\n",
    "   - High ratio = likely to be good, not yet explored\n",
    "   - This balances exploitation (areas we think are good) with exploration (uncertain areas)\n",
    "\n",
    "3. **Update models** after each evaluation\n",
    "   - More data → better models → smarter parameter selection\n",
    "\n",
    "**The key insight**: TPE doesn't just randomly search; it actively learns which regions of parameter space are promising.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Using Hyperopt with DBScan\n",
    "\n",
    "Previously, we used **Grid Search** to explore the hyperparameter space for DBSCAN.  \n",
    "Grid Search is easy to understand, but it doesn’t *learn* from past trials — every parameter setting is tested with no regard for what we’ve already learned.\n",
    "\n",
    "Now we’ll look at a smarter approach: **Bayesian Optimization** with [Hyperopt](https://github.com/hyperopt/hyperopt), which uses the **Tree-structured Parzen Estimator (TPE)** algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1 — Recap: Loss vs. Score\n",
    "\n",
    "Remember:\n",
    "\n",
    "- **scikit-learn GridSearchCV** expects a **score** (maximize).\n",
    "- **Hyperopt** expects a **loss** (minimize).\n",
    "\n",
    "If you already have a score you want to maximize, just negate it before returning.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2 — DBSCAN Coverage + Coherence\n",
    "\n",
    "We’ll reuse our **coverage** and **coherence** metrics from the previous section:\n",
    "\n",
    "- **Coverage:** fraction of points assigned to a cluster (`label != -1`).\n",
    "- **Coherence:** silhouette score of the non-noise points.\n",
    "\n",
    "We’ll combine them into a weighted average to form our **score**:\n",
    "\n",
    "$$\n",
    "\\text{score} = w \\times \\text{coverage} + (1-w) \\times \\text{coherence}\n",
    "$$\n",
    "\n",
    "We then turn that score into a **loss** for Hyperopt by negating it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Dataset: clusters + noise ---\n",
    "X_clusters, _ = make_blobs(\n",
    "    n_samples=2000, centers=6, cluster_std=0.70, random_state=42\n",
    ")\n",
    "X_noise = np.random.uniform(low=-8, high=8, size=(50, 2))\n",
    "X = np.vstack([X_clusters, X_noise])\n",
    "\n",
    "# --- Coverage & coherence metrics ---\n",
    "def coverage_score(labels):\n",
    "    return (labels != -1).mean()\n",
    "\n",
    "def coherence_score(X, labels):\n",
    "    mask = labels != -1\n",
    "    if mask.sum() < 2 or len(np.unique(labels[mask])) < 2:\n",
    "        return -1\n",
    "    return silhouette_score(X[mask], labels[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03293b56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3 — Defining the objective function for Hyperopt\n",
    "\n",
    "Hyperopt will repeatedly call this function with a set of parameters to test.  \n",
    "\n",
    "We:\n",
    "1. Fit DBSCAN with those parameters.\n",
    "2. Calculate coverage and coherence.\n",
    "3. Combine them into our weighted score.\n",
    "4. Convert to a **loss** by negating (because Hyperopt minimizes).\n",
    "5. Return the loss and a status flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_dbscan(params, X, coverage_weight=0.5, min_coverage=0.5):\n",
    "    eps = params['eps']\n",
    "    min_samples = int(params['min_samples'])\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    cov = coverage_score(labels)\n",
    "    coh = coherence_score(X, labels)\n",
    "    \n",
    "    if coh == -1:\n",
    "        score = -1\n",
    "    else:\n",
    "        score = coverage_weight * cov + (1 - coverage_weight) * coh\n",
    "    \n",
    "    # Optional: penalize low coverage\n",
    "    if cov < min_coverage:\n",
    "        penalty = (min_coverage - cov) * 2\n",
    "        score -= penalty\n",
    "    \n",
    "    return {'loss': -score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc7514",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4 — Defining the search space\n",
    "\n",
    "Hyperopt’s `hp` module lets us define:\n",
    "\n",
    "- **hp.uniform**: continuous range\n",
    "- **hp.quniform**: quantized range (integers)\n",
    "\n",
    "We’ll search over:\n",
    "- `eps`: 0.1 to 3.0\n",
    "- `min_samples`: integer between 3 and 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'eps': hp.uniform('eps', 0.1, 3.0),\n",
    "    'min_samples': hp.quniform('min_samples', 3, 20, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490420ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 5 — Simulating “initial random trials”\n",
    "\n",
    "Unlike some Bayesian optimization libraries, Hyperopt doesn’t have a built-in `init_points` argument.  \n",
    "If we want to start with random exploration, we can:\n",
    "\n",
    "- First run `fmin` with `algo=hyperopt.rand.suggest` for N iterations to gather random data.\n",
    "- Then continue with `algo=tpe.suggest` starting from the same `Trials` object.\n",
    "\n",
    "This is useful when:\n",
    "- You have no prior knowledge of the space.\n",
    "- You want TPE to start with a diverse set of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68774c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import rand\n",
    "\n",
    "# Store results across both phases\n",
    "trials = Trials()\n",
    "\n",
    "# Phase 1: random search for 10 trials\n",
    "fmin(\n",
    "    fn=lambda p: objective_dbscan(p, X, coverage_weight=0.5, min_coverage=0.7),\n",
    "    space=space,\n",
    "    algo=rand.suggest,\n",
    "    max_evals=10,\n",
    "    trials=trials,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Phase 2: TPE optimization for 40 trials\n",
    "fmin(\n",
    "    fn=lambda p: objective_dbscan(p, X, coverage_weight=0.8, min_coverage=0.7),\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,   # total trials = 10 random + 40 TPE\n",
    "    trials=trials,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48182df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6 — Inspecting results\n",
    "\n",
    "We can pull the best parameters and plot the optimization history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fce19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = trials.best_trial['result']\n",
    "best_score = -min([t['result']['loss'] for t in trials.trials])\n",
    "\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "print(\"Best parameters:\", trials.best_trial['misc']['vals'])\n",
    "\n",
    "# Plot optimization history\n",
    "losses = [-t['result']['loss'] for t in trials.trials]\n",
    "eps_vals = [t['misc']['vals']['eps'][0] for t in trials.trials]\n",
    "min_samples_vals = [t['misc']['vals']['min_samples'][0] for t in trials.trials]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss over iterations\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, marker='o')\n",
    "plt.axvline(9.5, color='red', linestyle='--', label='Switch to TPE')\n",
    "plt.title('Optimization Progress')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Coherence–Coverage Score')\n",
    "plt.legend()\n",
    "\n",
    "# Parameter exploration\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(eps_vals, min_samples_vals, c=losses, cmap='viridis')\n",
    "plt.colorbar(label='Score')\n",
    "plt.xlabel('eps')\n",
    "plt.ylabel('min_samples')\n",
    "plt.title('Parameter Space Exploration')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108076e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7 — Key takeaways\n",
    "\n",
    "- Hyperopt minimizes **loss**, so we negate scores when needed.\n",
    "- The **TPE algorithm** learns from past trials, focusing the search on promising areas.\n",
    "- We can simulate an “initial random exploration” phase by manually running `rand.suggest` before `tpe.suggest`.\n",
    "- The definition of the objective function (loss) is crucial — change the coverage weight and you’ll change the optimizer’s priorities.\n",
    "\n",
    "---\n",
    "\n",
    "📌 **Your turn**:\n",
    "- Change `coverage_weight` to 0.2 and rerun the search.\n",
    "- Observe how the “best” parameters change and how the search history looks.\n",
    "\n",
    "---\n",
    "\n",
    "## Comprehensive Example: Semantic Text Clustering with UMAP + HDBSCAN + Bayesian Optimization\n",
    "\n",
    "In this example, we’ll cluster **sentences** (not paragraphs) using semantic embeddings, optional dimensionality reduction, and a density‑based algorithm. We’ll demonstrate how to design a **loss function** that balances **coverage** (how many sentences get clustered) with **coherence** (how semantically tight clusters are). Then we’ll tune HDBSCAN with **Hyperopt** (TPE Bayesian optimization), including a short **random exploration** phase before TPE.\n",
    "\n",
    "**Pipeline overview:**\n",
    "1. Create several long paragraphs on different topics → **split into sentences**.\n",
    "2. Embed sentences with a **SentenceTransformer**.\n",
    "3. Precompute **three UMAP projections** (different `n_neighbors`/`min_dist`, always 2D). We let the optimizer **choose** the best projection rather than fully tuning UMAP (smaller search space, faster).\n",
    "4. Cluster using **HDBSCAN**.\n",
    "5. Define a **loss** that combines:\n",
    "   - **Coverage** = fraction of sentences assigned to any cluster (label != −1)\n",
    "   - **Coherence** = average **pairwise cosine similarity** inside each cluster (weighted by cluster size), computed from embeddings\n",
    "6. Use **Hyperopt** with a short **random** phase, then **TPE** to find good parameters.\n",
    "\n",
    "> This is conceptually similar to **BERTopic**: semantic embeddings → optional projection → density clustering, with a tunable “tightness vs. coverage” trade‑off. There are many valid ways to combine coverage and coherence; try alternatives (e.g., minimum coverage thresholds, unweighted averages) after the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, STATUS_OK, Trials\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425135d",
   "metadata": {},
   "source": [
    "### Step 1 — Build paragraphs, then split to sentences\n",
    "\n",
    "We’ll craft **four long paragraphs (≥20 sentences each)** across distinct topics. The goal is to generate some **semantically off‑topic sentences** that naturally become **noise** under the right clustering settings. After creating paragraphs, we **split into sentences** and keep the **topic label** for plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long paragraphs (each ≥20 sentences). Keep them short sentences for reliable splitting.\n",
    "tech_para = (\"Machine learning systems learn patterns from data. \"\n",
    "             \"Neural networks stack layers to extract features. \"\n",
    "             \"Transformers use attention to relate tokens. \"\n",
    "             \"Pretraining captures broad knowledge. \"\n",
    "             \"Finetuning adapts models to tasks. \"\n",
    "             \"Embedding spaces encode meaning. \"\n",
    "             \"Vector search retrieves similar items. \"\n",
    "             \"Inference speed matters in production. \"\n",
    "             \"Batching improves throughput. \"\n",
    "             \"Quantization reduces model size. \"\n",
    "             \"Distillation transfers knowledge. \"\n",
    "             \"Evaluation requires careful datasets. \"\n",
    "             \"Metrics must reflect objectives. \"\n",
    "             \"Biases can appear in data. \"\n",
    "             \"Safety requires constraints. \"\n",
    "             \"Monitoring detects drift. \"\n",
    "             \"Retraining restores performance. \"\n",
    "             \"Edge devices need efficient models. \"\n",
    "             \"Cloud scaling handles traffic. \"\n",
    "             \"Cost control guides deployment. \"\n",
    "             \"Documentation supports teams. \"\n",
    "             \"Experiment tracking preserves results.\")\n",
    "\n",
    "cooking_para = (\"Pasta water should be salty like the sea. \"\n",
    "                \"Fresh herbs brighten flavors. \"\n",
    "                \"Aromatics build complexity early. \"\n",
    "                \"Low heat coaxes sweetness from onions. \"\n",
    "                \"Proper searing develops fond. \"\n",
    "                \"Deglazing captures savory bits. \"\n",
    "                \"Emulsions need gradual fat addition. \"\n",
    "                \"Resting meat preserves juices. \"\n",
    "                \"Bread dough benefits from patience. \"\n",
    "                \"Gluten development adds structure. \"\n",
    "                \"Proofing time changes crumb. \"\n",
    "                \"Steam helps oven spring. \"\n",
    "                \"Thermometers prevent guesswork. \"\n",
    "                \"Acidity balances richness. \"\n",
    "                \"Salt enhances perception. \"\n",
    "                \"Textures should contrast. \"\n",
    "                \"Plating influences appetite. \"\n",
    "                \"Season every layer. \"\n",
    "                \"Waste less by reusing scraps. \"\n",
    "                \"Stocks concentrate flavor. \"\n",
    "                \"Simple dishes demand technique. \"\n",
    "                \"Practice grows intuition.\")\n",
    "\n",
    "sports_para = (\"Endurance builds with consistent training. \"\n",
    "               \"Recovery makes adaptation possible. \"\n",
    "               \"Hydration supports performance. \"\n",
    "               \"Footwork dictates positioning. \"\n",
    "               \"Coaches design effective drills. \"\n",
    "               \"Teamwork creates passing lanes. \"\n",
    "               \"Defensive spacing reduces chances. \"\n",
    "               \"Strength training prevents injury. \"\n",
    "               \"Mobility enhances range of motion. \"\n",
    "               \"Warmups prime the nervous system. \"\n",
    "               \"Cooldowns aid circulation. \"\n",
    "               \"Video analysis reveals tendencies. \"\n",
    "               \"Strategy varies by opponent. \"\n",
    "               \"Tempo changes unsettle defenses. \"\n",
    "               \"Composure matters under pressure. \"\n",
    "               \"Sleep improves reaction time. \"\n",
    "               \"Nutrition fuels workloads. \"\n",
    "               \"Periodization structures seasons. \"\n",
    "               \"Mindset shapes execution. \"\n",
    "               \"Fundamentals win close games. \"\n",
    "               \"Practice quality beats volume. \"\n",
    "               \"Confidence grows from preparation.\")\n",
    "\n",
    "finance_para = (\"Diversification reduces idiosyncratic risk. \"\n",
    "                \"Correlation patterns shift over cycles. \"\n",
    "                \"Liquidity dries up in stress. \"\n",
    "                \"Volatility clusters in regimes. \"\n",
    "                \"Risk premia are time varying. \"\n",
    "                \"Macro surprises move markets. \"\n",
    "                \"Earnings anchor valuations. \"\n",
    "                \"Discount rates reflect policy. \"\n",
    "                \"Inflation reshapes cash flows. \"\n",
    "                \"Term structure signals expectations. \"\n",
    "                \"Credit spreads price default risk. \"\n",
    "                \"Hedging trades basis risks. \"\n",
    "                \"Leverage amplifies drawdowns. \"\n",
    "                \"Diversified funding stabilizes firms. \"\n",
    "                \"Capital buffers absorb losses. \"\n",
    "                \"Position sizing controls exposure. \"\n",
    "                \"Stop losses enforce discipline. \"\n",
    "                \"Rebalancing locks in drift. \"\n",
    "                \"Transaction costs erode alpha. \"\n",
    "                \"Governance influences outcomes. \"\n",
    "                \"Regulation alters incentives. \"\n",
    "                \"Data quality drives decisions.\")\n",
    "\n",
    "def split_sentences(paragraph, topic):\n",
    "    # Simple sentence split (works well for our short sentences)\n",
    "    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', paragraph) if s.strip()]\n",
    "    return pd.DataFrame({\"text\": sentences, \"topic\": topic})\n",
    "\n",
    "df = pd.concat([\n",
    "    split_sentences(tech_para, \"tech\"),\n",
    "    split_sentences(cooking_para, \"cooking\"),\n",
    "    split_sentences(sports_para, \"sports\"),\n",
    "    split_sentences(finance_para, \"finance\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "print(df.head(8))\n",
    "print(f\"\\nTotal sentences: {len(df)}  | by topic: {df['topic'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f56915",
   "metadata": {},
   "source": [
    "### Step 2 — Embed sentences\n",
    "\n",
    "We’ll use a compact, widely used model: `all-MiniLM-L6-v2`. Replace with your preferred model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(texts, show_progress_bar=False, normalize_embeddings=True)\n",
    "\n",
    "embeddings = embed_texts(df[\"text\"].tolist())\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95703232",
   "metadata": {},
   "source": [
    "### Step 3 — Precompute three UMAP projections (2D)\n",
    "\n",
    "We’ll create **three** fixed UMAP settings and precompute the 2D projections. The optimizer will **choose** among them.  \n",
    "You *could* include UMAP’s parameters directly in the search, but that expands the search space and runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39789a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_configs = [\n",
    "    {\"n_neighbors\": 5,  \"min_dist\": 0.05},\n",
    "    {\"n_neighbors\": 10, \"min_dist\": 0.10},\n",
    "    {\"n_neighbors\": 25, \"min_dist\": 0.25},\n",
    "]\n",
    "\n",
    "umap_projections = []\n",
    "for cfg in umap_configs:\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=cfg[\"n_neighbors\"],\n",
    "        min_dist=cfg[\"min_dist\"],\n",
    "        n_components=2,\n",
    "        random_state=42,\n",
    "        metric=\"cosine\",  # cosine often works well for sentence embeddings\n",
    "    )\n",
    "    umap_projections.append(reducer.fit_transform(embeddings))\n",
    "\n",
    "[len(p) for p in umap_projections]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd8fee",
   "metadata": {},
   "source": [
    "### Step 4 — Define coverage and semantic coherence\n",
    "\n",
    "- **Coverage:** fraction of sentences assigned to a cluster (`label != -1`).\n",
    "- **Coherence (semantic):** for each cluster, compute the **pairwise cosine similarity matrix** for its member embeddings, take the **upper triangle mean**, and aggregate to a **global score weighted by cluster size**.\n",
    "\n",
    "> Why weighted? Larger clusters should contribute proportionally more to global coherence. Try an unweighted average as an alternative exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a024d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_score(labels):\n",
    "    labels = np.asarray(labels)\n",
    "    return (labels != -1).mean()\n",
    "\n",
    "def cluster_coherence_from_embeddings(embeds, labels):\n",
    "    \"\"\"\n",
    "    Return (global_coherence, per_cluster_dict).\n",
    "    global_coherence is the size-weighted mean of per-cluster mean cosine similarity.\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() < 2 or len(np.unique(labels[mask])) < 1:\n",
    "        return -1.0, {}\n",
    "\n",
    "    per_cluster = {}\n",
    "    totals = 0\n",
    "    weighted_sum = 0.0\n",
    "\n",
    "    for c in np.unique(labels[mask]):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        if len(idx) < 2:\n",
    "            per_cluster[int(c)] = -1.0\n",
    "            continue\n",
    "        sims = cosine_similarity(embeds[idx], embeds[idx])\n",
    "        # take strict upper triangle to avoid self-similarity\n",
    "        ut = np.triu_indices_from(sims, k=1)\n",
    "        vals = sims[ut]\n",
    "        if len(vals) == 0:\n",
    "            per_cluster[int(c)] = -1.0\n",
    "            continue\n",
    "        coh = float(np.mean(vals))\n",
    "        per_cluster[int(c)] = coh\n",
    "        weighted_sum += coh * len(idx)\n",
    "        totals += len(idx)\n",
    "\n",
    "    if totals == 0:\n",
    "        return -1.0, per_cluster\n",
    "\n",
    "    global_coh = weighted_sum / totals\n",
    "    # Rescale to [0,1] for visual comparability (optional, cosine sim already ~[0,1])\n",
    "    return float(global_coh), per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbdb830",
   "metadata": {},
   "source": [
    "### Step 5 — Define the objective (loss) for Hyperopt\n",
    "\n",
    "We combine **coverage** and **coherence** into a single **score**, then negate it to produce a **loss** (Hyperopt minimizes).  \n",
    "We also:\n",
    "- Include a **minimum coverage** soft constraint with a penalty.\n",
    "- Allow Hyperopt to **choose among three UMAP projections** via a categorical index.\n",
    "- Tune HDBSCAN’s `min_cluster_size` and `min_samples`.\n",
    "\n",
    "**Score definition:**\n",
    "\n",
    "$$\n",
    "\\text{score} = w \\cdot \\text{coverage} + (1-w) \\cdot \\text{coherence}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(umap_projections, embeddings, coverage_weight=0.5, min_coverage=0.5):\n",
    "    # Note: coherence is computed from the original embeddings (semantic space),\n",
    "    # while clustering happens in 2D (chosen UMAP projection).\n",
    "    def objective(params):\n",
    "        umap_ix = int(params[\"umap_ix\"])\n",
    "        min_cluster_size = int(params[\"min_cluster_size\"])\n",
    "        min_samples = int(params[\"min_samples\"])\n",
    "\n",
    "        coords_2d = umap_projections[umap_ix]\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            cluster_selection_epsilon=0.0,\n",
    "            metric=\"euclidean\"  # on UMAP space\n",
    "        )\n",
    "        labels = clusterer.fit_predict(coords_2d)\n",
    "\n",
    "        cov = coverage_score(labels)\n",
    "        coh, _ = cluster_coherence_from_embeddings(embeddings, labels)\n",
    "\n",
    "        if coh < 0:\n",
    "            score = -1.0\n",
    "        else:\n",
    "            score = coverage_weight * cov + (1 - coverage_weight) * coh\n",
    "\n",
    "        # Soft penalty for not meeting coverage target\n",
    "        if cov < min_coverage:\n",
    "            score -= (min_coverage - cov) * 2.0\n",
    "\n",
    "        return {\"loss\": -score, \"status\": STATUS_OK, \"labels\": labels, \"umap_ix\": umap_ix}\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183cf22",
   "metadata": {},
   "source": [
    "## Step 6 — Hyperopt search (random → TPE)\n",
    "\n",
    "We simulate an **initial random phase** (10 trials), then continue with **TPE** for smarter exploration.  \n",
    "\n",
    "Search space:\n",
    "- `umap_ix` ∈ {0,1,2} (choose one of the precomputed UMAPs)\n",
    "- `min_cluster_size` ∈ [3, 20]\n",
    "- `min_samples` ∈ [1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "space = {\n",
    "    \"umap_ix\": hp.choice(\"umap_ix\", [0, 1, 2]),\n",
    "    \"min_cluster_size\": hp.quniform(\"min_cluster_size\", 3, 20, 1),\n",
    "    \"min_samples\": hp.quniform(\"min_samples\", 1, 10, 1),\n",
    "}\n",
    "\n",
    "coverage_weight = 0.5   # try different numbers later!\n",
    "min_coverage   = 0.2\n",
    "\n",
    "objective = make_objective(umap_projections, embeddings,\n",
    "                           coverage_weight=coverage_weight,\n",
    "                           min_coverage=min_coverage)\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# Phase 1: random\n",
    "fmin(fn=objective, space=space, algo=rand.suggest,\n",
    "     max_evals=10, trials=trials, verbose=True)\n",
    "\n",
    "# Phase 2: TPE\n",
    "fmin(fn=objective, space=space, algo=tpe.suggest,\n",
    "     max_evals=50, trials=trials, verbose=True)\n",
    "\n",
    "# Extract best trial\n",
    "best_trial = min(trials.trials, key=lambda t: t[\"result\"][\"loss\"])\n",
    "best_umap_ix = best_trial[\"result\"][\"umap_ix\"]\n",
    "best_labels  = best_trial[\"result\"][\"labels\"]\n",
    "best_coords  = umap_projections[best_umap_ix]\n",
    "\n",
    "best_cov = coverage_score(best_labels)\n",
    "best_coh, per_cluster = cluster_coherence_from_embeddings(embeddings, best_labels)\n",
    "best_score = coverage_weight * best_cov + (1 - coverage_weight) * best_coh\n",
    "\n",
    "print(f\"Best UMAP index: {best_umap_ix}  | config: {umap_configs[best_umap_ix]}\")\n",
    "print(f\"Coverage: {best_cov:.3f}  | Coherence: {best_coh:.3f}  | Combined score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f01825",
   "metadata": {},
   "source": [
    "## Step 7 — Visualize\n",
    "\n",
    "- **Color** encodes the **paragraph topic** (tech/cooking/sports/finance).\n",
    "- **Alpha** distinguishes **noise** (low alpha) from clustered points (high alpha).\n",
    "- **Optional**: draw **convex hulls** around each discovered cluster to make cluster boundaries visible (skip clusters with < 3 points).\n",
    "\n",
    "> Using color for topic lets you see whether clusters align with semantic origin. Using alpha for noise keeps color free for topic identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fba0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_color = {\"tech\": \"#1f77b4\", \"cooking\": \"#2ca02c\", \"sports\": \"#ff7f0e\", \"finance\": \"#9467bd\"}\n",
    "colors = df[\"topic\"].map(topic_to_color).values\n",
    "\n",
    "def plot_clusters(coords, labels, colors, draw_hulls=True, title=\"UMAP + HDBSCAN (semantic topics shown)\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    labels = np.asarray(labels)\n",
    "    unique_labels = sorted([l for l in np.unique(labels) if l != -1])\n",
    "\n",
    "    # Noise first (low alpha)\n",
    "    noise_mask = labels == -1\n",
    "    plt.scatter(coords[noise_mask, 0], coords[noise_mask, 1],\n",
    "                c=colors[noise_mask], alpha=0.25, s=30, label=\"Noise\")\n",
    "\n",
    "    # Clusters (higher alpha)\n",
    "    for c in unique_labels:\n",
    "        mask = labels == c\n",
    "        plt.scatter(coords[mask, 0], coords[mask, 1],\n",
    "                    c=colors[mask], alpha=0.9, s=35, label=f\"Cluster {c}\")\n",
    "\n",
    "        # Optional convex hull outline\n",
    "        if draw_hulls and mask.sum() >= 3:\n",
    "            try:\n",
    "                hull = ConvexHull(coords[mask])\n",
    "                hull_pts = coords[mask][hull.vertices]\n",
    "                plt.plot(np.append(hull_pts[:,0], hull_pts[0,0]),\n",
    "                         np.append(hull_pts[:,1], hull_pts[0,1]),\n",
    "                         linestyle='-', linewidth=1.5, color='black', alpha=0.6)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    # Custom legend: topic colors + noise entry\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_items = [Line2D([0],[0], marker='o', color='w', label=t,\n",
    "                           markerfacecolor=topic_to_color[t], markersize=8)\n",
    "                    for t in topic_to_color]\n",
    "    legend_items.insert(0, Line2D([0],[0], marker='o', color='w', label='Noise',\n",
    "                                  markerfacecolor='gray', markeredgecolor='gray', alpha=0.3, markersize=8))\n",
    "    plt.legend(handles=legend_items, loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(best_coords, best_labels, colors, draw_hulls=True,\n",
    "              title=f\"Best projection index {best_umap_ix} — clusters outlined, noise faded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c04bf7",
   "metadata": {},
   "source": [
    "### Step 8 — Extensions and exercises\n",
    "\n",
    "- **Change the trade‑off**: set `coverage_weight` to 0.2 (favor coherence) or 0.8 (favor coverage) and rerun. How do coverage and coherence respond?\n",
    "- **Different aggregation**: try *unweighted* average of cluster coherences, or a **minimum coverage threshold** without weighting.\n",
    "- **Alternative metrics**: experiment with other intra‑cluster similarity measures (e.g., median instead of mean; exclude outlier pairs).\n",
    "- **UMAP in the optimizer**: replace the fixed `umap_ix` choice with continuous `n_neighbors` and `min_dist` in the search space. This increases compute but may find better optima.\n",
    "- **HDBSCAN options**: tune `cluster_selection_epsilon` or `cluster_selection_method`.\n",
    "- **Compare to BERTopic**: BERTopic automates many of these steps; after this lab, inspect how its parameters map onto the same trade‑offs.\n",
    "\n",
    "**Key idea:** In unsupervised tasks, the *definition of the objective* (loss/score) encodes what “good” means. Your optimization is only as good as that definition.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Guidelines\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "1. **Grid Search** — Small, low-dimensional spaces where you want exhaustive coverage.\n",
    "2. **Random Search** — Medium spaces or limited compute; strong baseline that often beats grid for the same budget.\n",
    "3. **Bayesian Optimization (Hyperopt/TPE)** — Larger/complex spaces or expensive evaluations; learns from previous trials.\n",
    "4. **Multi-objective Search** — When you explicitly need to balance **coverage vs. coherence** (e.g., by weighted sums or simple constraints like minimum coverage).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Considerations for Clustering Hyperparameter Tuning\n",
    "\n",
    "1. **Define Clear Objectives** (what “good” means for *your* task)\n",
    "   - **Cluster quality** (e.g., silhouette, Davies–Bouldin; be mindful of biases toward spherical clusters).\n",
    "   - **Coverage** (fraction of points assigned to clusters).\n",
    "   - **Interpretability & domain relevance** (are clusters useful to humans?).\n",
    "   - **Stability** (do results persist across reruns, perturbations, or small data changes?).\n",
    "\n",
    "2. **Handle Incomplete Clustering** (DBSCAN, HDBSCAN)\n",
    "   - Set a **minimum coverage** requirement or add a **penalty** when coverage is low.\n",
    "   - Treat “noise” points as a feature, not a bug (they can signal outliers or off-topic content).\n",
    "   - Align your metric with intent: it’s fine to accept lower coverage if your priority is very tight, coherent clusters.\n",
    "\n",
    "3. **Validate Thoroughly — Without Ground Truth**\n",
    "   - **Bootstrap stability analysis (what it means here):**  \n",
    "     Refit your pipeline multiple times under small changes (e.g., resample sentences with replacement; jitter embeddings slightly; change UMAP seeds; re-run HDBSCAN).  \n",
    "     Compare runs using **permutation-invariant** measures (e.g., pairwise co-assignment matrices, Adjusted Rand Index / NMI on aligned labels).  \n",
    "     *Goal:* stable clusters across perturbations.\n",
    "   - **Cross-validation for clustering (in this context):**  \n",
    "     Split data (train/hold-out). Fit clusters on train. Then **assign** hold-out points to clusters (e.g., nearest centroid in embedding space, `hdbscan.approximate_predict`, or a k-NN to cluster cores).  \n",
    "     Evaluate hold-out **coherence** (e.g., mean in-cluster cosine similarity) and **coverage**.  \n",
    "     *Goal:* clusters generalize beyond the data used to form them.\n",
    "   - **Visual inspection with DR:**  \n",
    "     Use UMAP/TSNE/PCAs for sanity checks—look for structure that matches metrics.\n",
    "   - **Domain expert review:**  \n",
    "     Short summaries/examples per cluster; assess usefulness and face validity.\n",
    "\n",
    "4. **Scale (and Metric) Appropriately**\n",
    "   - Distance choices matter: **cosine** is often better for text embeddings; Euclidean can be sensitive to scale.\n",
    "   - If using Euclidean methods (e.g., k-means, Euclidean DBSCAN), apply **standardization**; for cosine, **L2-normalize** embeddings.\n",
    "   - Verify scaling/metric choices with domain knowledge (e.g., cosine for sentences).\n",
    "\n",
    "5. **Computational Efficiency**\n",
    "   - Set **time/iteration budgets** and adopt **early stopping** (e.g., stop after N no-improvement trials).\n",
    "   - **Cache** expensive steps (embeddings, UMAP projections, distance matrices).\n",
    "   - **Parallelize** where possible (e.g., `n_jobs`, batched inference).\n",
    "   - Start simple (smaller grids / fewer trials), then expand if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "1. **Over-optimizing internal metrics** (e.g., silhouette favors convex clusters; don’t chase a single number).\n",
    "2. **Ignoring domain knowledge** (a high score ≠ useful clusters).\n",
    "3. **Skipping stability checks** (results should persist under small changes).\n",
    "4. **Treating noise as failure** (noise can be informative).\n",
    "5. **Using the wrong distance/scale** (mismatch between data geometry and metric undermines results).\n",
    "\n",
    "The art of unsupervised tuning is balancing multiple objectives while staying grounded in domain goals. Start simple, **validate stability and generalization**, and always inspect results visually and conceptually.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Decision Framework: Which Search Method to Use?\n",
    "\n",
    "### Decision Tree for Method Selection\n",
    "\n",
    "Clustering Hyperparameter Tuning Decision Framework:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0daf8",
   "metadata": {},
   "source": [
    "```text\n",
    "\n",
    "1. How many hyperparameters?\n",
    "   ├─ 1–2 → Manual exploration or Grid Search\n",
    "   ├─ 3–4 → Random Search (Grid if budget allows)\n",
    "   └─ 5+  → Random Search or Bayesian Optimization\n",
    "\n",
    "2. How expensive is each evaluation?\n",
    "   ├─ Fast (< 1s)      → Grid or Random\n",
    "   ├─ Medium (1–60s)   → Random or Bayesian (TPE)\n",
    "   └─ Slow (> 1 min)   → Bayesian (TPE), small budget\n",
    "\n",
    "3. How much domain knowledge?\n",
    "   ├─ High   → Manual exploration → local optimization\n",
    "   ├─ Medium → Random with informed ranges\n",
    "   └─ Low    → Bayesian to learn relationships\n",
    "\n",
    "4. What’s the compute budget?\n",
    "   ├─ Limited (< 50 evals)   → Bayesian (TPE)\n",
    "   ├─ Moderate (50–500)      → Random\n",
    "   └─ High (> 500, low-dim)  → Grid\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cbf3b",
   "metadata": {},
   "source": [
    "### Example Scenarios\n",
    "\n",
    "**Scenario 1: Quick K-means tuning**  \n",
    "1 parameter (`k`), very fast.  \n",
    "**Use:** Manual elbow or tiny grid.\n",
    "\n",
    "**Scenario 2: DBSCAN on a moderate dataset**  \n",
    "2 parameters (`eps`, `min_samples`), medium cost.  \n",
    "**Use:** Random search or TPE (≈30–50 evals).\n",
    "\n",
    "**Scenario 3: UMAP + HDBSCAN on large text**  \n",
    "4+ parameters, slow (embeddings + DR).  \n",
    "**Use:** TPE with a small, carefully chosen budget (e.g., 20–30 evals), cache embeddings/UMAP.\n",
    "\n",
    "**Scenario 4: Real-time re-tuning**  \n",
    "Frequent updates, tight latency.  \n",
    "**Use:** Fast random search + early stopping; reuse caches.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts: The Meta-Learning Perspective\n",
    "\n",
    "As you practice, you’ll build intuition about:\n",
    "1. **Parameter interactions** (e.g., how `eps` and `min_samples` trade off in DBSCAN).\n",
    "2. **Data characteristics** (e.g., high dimensionality favors cosine; scale matters).\n",
    "3. **Domain patterns** (typical ranges that work for *your* tasks).\n",
    "4. **Evaluation trade-offs** (when coverage should trump coherence—and vice versa).\n",
    "\n",
    "This becomes part of your **domain expertise**—the secret sauce that turns generic optimizers into problem-solving tools. Remember: perfection isn’t the target; **actionable, trustworthy clusters** are. A “pretty good” clustering that experts can use often beats a “perfect” one that they can’t interpret."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
